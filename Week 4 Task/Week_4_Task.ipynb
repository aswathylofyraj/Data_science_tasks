{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYXeSQDKnPKZ",
        "outputId": "da1301b5-a1bf-4f7c-8c0a-0f0c9801a74b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Install and import libraries\n",
        "import nltk\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "# Download required resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt_tab')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "file_path = \"/content/spam_sms.csv\"  # Replace with your actual file path\n",
        "data = pd.read_csv(file_path, encoding='latin-1')\n"
      ],
      "metadata": {
        "id": "ijoU0Lo4o9K0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the dataset\n",
        "print(\"First few rows of the dataset:\")\n",
        "print(data.head())\n",
        "\n",
        "# Rename columns for clarity\n",
        "data.columns = ['Label', 'Message']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EndyZ6zspGEi",
        "outputId": "4a28b8dc-6370-4a5a-e86e-38e137740ee6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First few rows of the dataset:\n",
            "     v1                                                 v2\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values\n",
        "print(\"\\nChecking for missing values:\")\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Drop missing values if any\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Map labels to binary values\n",
        "data['Label'] = data['Label'].map({'ham': 0, 'spam': 1})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8onyaTj-pRW5",
        "outputId": "508c500c-a730-48ad-af2d-0003ddb0f3af"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking for missing values:\n",
            "Label      0\n",
            "Message    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the text data\n",
        "def clean_text(text):\n",
        "    # Remove special characters, numbers, and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "data['Cleaned_Message'] = data['Message'].apply(clean_text)"
      ],
      "metadata": {
        "id": "uUbQG_a5pZXd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "data['Tokenized_Message'] = data['Cleaned_Message'].apply(word_tokenize)\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "data['Tokenized_Message'] = data['Tokenized_Message'].apply(\n",
        "    lambda tokens: [word for word in tokens if word not in stop_words]\n",
        ")\n",
        "\n",
        "# Apply stemming\n",
        "stemmer = PorterStemmer()\n",
        "data['Stemmed_Message'] = data['Tokenized_Message'].apply(\n",
        "    lambda tokens: [stemmer.stem(word) for word in tokens]\n",
        ")\n",
        "\n",
        "# Apply lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "data['Lemmatized_Message'] = data['Tokenized_Message'].apply(\n",
        "    lambda tokens: [lemmatizer.lemmatize(word) for word in tokens]\n",
        ")"
      ],
      "metadata": {
        "id": "jn1Gjl5XpdrA"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data['Lemmatized_Message'], data['Label'], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Display final processed data\n",
        "print(\"\\nSample of processed data:\")\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NvQQzTs9GCm9",
        "outputId": "72539f80-cd66-41ba-88a3-65400f51e3b5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample of processed data:\n",
            "   Label                                            Message  \\\n",
            "0      0  Go until jurong point, crazy.. Available only ...   \n",
            "1      0                      Ok lar... Joking wif u oni...   \n",
            "2      1  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
            "3      0  U dun say so early hor... U c already then say...   \n",
            "4      0  Nah I don't think he goes to usf, he lives aro...   \n",
            "\n",
            "                                     Cleaned_Message  \\\n",
            "0  go until jurong point crazy available only in ...   \n",
            "1                            ok lar joking wif u oni   \n",
            "2  free entry in  a wkly comp to win fa cup final...   \n",
            "3        u dun say so early hor u c already then say   \n",
            "4  nah i dont think he goes to usf he lives aroun...   \n",
            "\n",
            "                                   Tokenized_Message  \\\n",
            "0  [go, jurong, point, crazy, available, bugis, n...   \n",
            "1                     [ok, lar, joking, wif, u, oni]   \n",
            "2  [free, entry, wkly, comp, win, fa, cup, final,...   \n",
            "3      [u, dun, say, early, hor, u, c, already, say]   \n",
            "4  [nah, dont, think, goes, usf, lives, around, t...   \n",
            "\n",
            "                                     Stemmed_Message  \\\n",
            "0  [go, jurong, point, crazi, avail, bugi, n, gre...   \n",
            "1                       [ok, lar, joke, wif, u, oni]   \n",
            "2  [free, entri, wkli, comp, win, fa, cup, final,...   \n",
            "3      [u, dun, say, earli, hor, u, c, alreadi, say]   \n",
            "4  [nah, dont, think, goe, usf, live, around, tho...   \n",
            "\n",
            "                                  Lemmatized_Message  \n",
            "0  [go, jurong, point, crazy, available, bugis, n...  \n",
            "1                     [ok, lar, joking, wif, u, oni]  \n",
            "2  [free, entry, wkly, comp, win, fa, cup, final,...  \n",
            "3      [u, dun, say, early, hor, u, c, already, say]  \n",
            "4  [nah, dont, think, go, usf, life, around, though]  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lemmatized messages back into strings for vectorization\n",
        "data['Processed_Message'] = data['Lemmatized_Message'].apply(lambda tokens: ' '.join(tokens))\n",
        "\n",
        "# Apply TF-IDF Vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Adjust max_features if needed\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(data['Processed_Message'])\n",
        "\n",
        "\n",
        "# Convert the sparse matrices to DataFrames for easier manipulation\n",
        "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "\n",
        "# Add the labels back to the data\n",
        "tfidf_df['Label'] = data['Label'].values\n",
        "\n",
        "\n",
        "# Save the datasets to CSV files\n",
        "tfidf_df.to_csv('tfidf_sms_dataset.csv', index=False)\n",
        "\n",
        "\n",
        "print(\"TF-IDF datasets saved as 'tfidf_sms_dataset.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BLiQ4wTKfLJ",
        "outputId": "66545243-10e0-4443-c69f-914b141122d3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF datasets saved as 'tfidf_sms_dataset.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the TF-IDF dataset\n",
        "tfidf_df = pd.read_csv('tfidf_sms_dataset.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = tfidf_df.drop(columns=['Label'])\n",
        "y = tfidf_df['Label']\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Model Performance Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8KCDJwkzNAD",
        "outputId": "ef93b7eb-7e66-4fe1-9fe6-9186cd4f6a64"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Performance Metrics:\n",
            "Accuracy: 0.9480\n",
            "Precision: 0.9600\n",
            "Recall: 0.6400\n",
            "F1-Score: 0.7680\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.97       965\n",
            "           1       0.96      0.64      0.77       150\n",
            "\n",
            "    accuracy                           0.95      1115\n",
            "   macro avg       0.95      0.82      0.87      1115\n",
            "weighted avg       0.95      0.95      0.94      1115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the TF-IDF dataset\n",
        "tfidf_df = pd.read_csv('tfidf_sms_dataset.csv')\n",
        "\n",
        "# Separate features and labels\n",
        "X = tfidf_df.drop(columns=['Label']).values\n",
        "y = tfidf_df['Label'].values\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize the data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential([\n",
        "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')  # Sigmoid for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, verbose=2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Make predictions\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate performance\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2xnrcbfzpBJ",
        "outputId": "d7aeeb82-1d3a-4e06-f210-e35dfd18f242"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "112/112 - 5s - 40ms/step - accuracy: 0.8898 - loss: 0.2882 - val_accuracy: 0.9652 - val_loss: 0.1176\n",
            "Epoch 2/10\n",
            "112/112 - 1s - 10ms/step - accuracy: 0.9818 - loss: 0.0609 - val_accuracy: 0.9798 - val_loss: 0.0834\n",
            "Epoch 3/10\n",
            "112/112 - 2s - 15ms/step - accuracy: 0.9969 - loss: 0.0122 - val_accuracy: 0.9776 - val_loss: 0.0769\n",
            "Epoch 4/10\n",
            "112/112 - 2s - 20ms/step - accuracy: 0.9992 - loss: 0.0052 - val_accuracy: 0.9776 - val_loss: 0.0801\n",
            "Epoch 5/10\n",
            "112/112 - 2s - 21ms/step - accuracy: 0.9994 - loss: 0.0030 - val_accuracy: 0.9776 - val_loss: 0.0817\n",
            "Epoch 6/10\n",
            "112/112 - 1s - 13ms/step - accuracy: 0.9994 - loss: 0.0018 - val_accuracy: 0.9787 - val_loss: 0.0887\n",
            "Epoch 7/10\n",
            "112/112 - 3s - 25ms/step - accuracy: 0.9994 - loss: 0.0020 - val_accuracy: 0.9776 - val_loss: 0.0912\n",
            "Epoch 8/10\n",
            "112/112 - 1s - 12ms/step - accuracy: 0.9997 - loss: 0.0011 - val_accuracy: 0.9753 - val_loss: 0.0858\n",
            "Epoch 9/10\n",
            "112/112 - 2s - 22ms/step - accuracy: 1.0000 - loss: 5.9912e-04 - val_accuracy: 0.9742 - val_loss: 0.0840\n",
            "Epoch 10/10\n",
            "112/112 - 1s - 11ms/step - accuracy: 1.0000 - loss: 5.8213e-04 - val_accuracy: 0.9753 - val_loss: 0.0845\n",
            "Test Accuracy: 0.9731\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98       965\n",
            "           1       0.92      0.88      0.90       150\n",
            "\n",
            "    accuracy                           0.97      1115\n",
            "   macro avg       0.95      0.93      0.94      1115\n",
            "weighted avg       0.97      0.97      0.97      1115\n",
            "\n"
          ]
        }
      ]
    }
  ]
}